{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede Neural 'do Zero' para Classificação de Uvas-Passas\n",
    "\n",
    "Este notebook implementa uma rede neural para um problema de classificação binária usando principalmente a biblioteca `NumPy` para as operações do modelo. O objetivo é classificar duas variedades de uvas-passas (Kecimen e Besni) com base em 7 características morfológicas.\n",
    "\n",
    "**Etapas do Projeto:**\n",
    "1. **Preparação dos Dados**: Carregar, explorar, codificar e escalar os dados.\n",
    "2. **Implementação da Rede Neural**: Criar uma classe `NeuralNetwork` com a lógica de *forward propagation*, *cálculo de custo*, *backpropagation* e *atualização de pesos*.\n",
    "3. **Treinamento**: Executar o loop de treinamento para ajustar os pesos do modelo.\n",
    "4. **Avaliação de Desempenho**: Avaliar a performance do modelo treinado com métricas padrão e visualizações gráficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 1: Importação de Bibliotecas e Preparação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_excel('Raisin_Dataset.xlsx')\n",
    "    print(\"Dataset carregado com sucesso!\")\n",
    "    print(\"\\nAmostra dos dados:\")\n",
    "    display(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"ERRO: O arquivo 'Raisin_Dataset.csv' não foi encontrado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pré-processamento dos Dados ---\n",
    "\n",
    "# 1. Separar as features (características) do alvo (classe)\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# 2. Codificar o alvo: Converter as classes 'Besni' e 'Kecimen' para 0 e 1\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "print(f\"Classes originais: {label_encoder.classes_}\")\n",
    "print(f\"Classes codificadas para: {np.unique(y_encoded)}\\n\")\n",
    "\n",
    "# 3. Dividir os dados em conjuntos de treino e teste (80% para treino, 20% para teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# 4. Padronizar as features: essencial para o bom desempenho de redes neurais\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Ajustar as dimensões dos dados para a nossa implementação manual\n",
    "# A rede espera que X tenha o formato (n_features, n_amostras) e y tenha (1, n_amostras)\n",
    "X_train_T = X_train_scaled.T\n",
    "X_test_T = X_test_scaled.T\n",
    "y_train_T = y_train.reshape(1, -1)\n",
    "y_test_T = y_test.reshape(1, -1)\n",
    "\n",
    "print(f\"Dimensão de X de treino (features, amostras): {X_train_T.shape}\")\n",
    "print(f\"Dimensão de y de treino (1, amostras): {y_train_T.shape}\")\n",
    "print(f\"Dimensão de X de teste (features, amostras): {X_test_T.shape}\")\n",
    "print(f\"Dimensão de y de teste (1, amostras): {y_test_T.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 2: Implementação da Classe NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Implementação de uma rede neural de 2 camadas (1 oculta + 1 de saída) com NumPy.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims):\n",
    "        self.parameters = {}\n",
    "        self.layer_dims = layer_dims\n",
    "        # Inicializa os pesos com valores pequenos e aleatórios e os vieses com zero\n",
    "        for l in range(1, len(layer_dims)):\n",
    "            self.parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "            self.parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    def _sigmoid(self, Z):\n",
    "        \"\"\"Função de ativação Sigmoid.\"\"\"\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def _relu(self, Z):\n",
    "        \"\"\"Função de ativação ReLU.\"\"\"\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Executa a passagem para frente (da entrada para a saída).\"\"\"\n",
    "        cache = {}\n",
    "        A = X\n",
    "        cache['A0'] = X\n",
    "        \n",
    "        # Camada Oculta (Ativação ReLU)\n",
    "        Z1 = np.dot(self.parameters['W1'], A) + self.parameters['b1']\n",
    "        A1 = self._relu(Z1)\n",
    "        cache['Z1'] = Z1\n",
    "        cache['A1'] = A1\n",
    "        \n",
    "        # Camada de Saída (Ativação Sigmoid para classificação binária)\n",
    "        Z2 = np.dot(self.parameters['W2'], A1) + self.parameters['b2']\n",
    "        A2 = self._sigmoid(Z2)\n",
    "        cache['Z2'] = Z2\n",
    "        cache['A2'] = A2\n",
    "        \n",
    "        return A2, cache\n",
    "\n",
    "    def compute_cost(self, A2, Y):\n",
    "        \"\"\"Calcula o custo usando a Entropia Cruzada Binária.\"\"\"\n",
    "        m = Y.shape[1] # número de amostras\n",
    "        cost = - (1/m) * np.sum(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8)) # Adicionado 1e-8 para estabilidade numérica\n",
    "        return np.squeeze(cost)\n",
    "\n",
    "    def backward_propagation(self, Y, cache):\n",
    "        \"\"\"Executa a retropropagação para calcular os gradientes do erro.\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        grads = {}\n",
    "        \n",
    "        A0, A1, A2 = cache['A0'], cache['A1'], cache['A2']\n",
    "        Z1 = cache['Z1']\n",
    "\n",
    "        # Gradientes para a Camada de Saída\n",
    "        dZ2 = A2 - Y\n",
    "        grads['dW2'] = (1/m) * np.dot(dZ2, A1.T)\n",
    "        grads['db2'] = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        # Gradientes para a Camada Oculta\n",
    "        drelu_dz1 = (Z1 > 0).astype(int) # Derivada da ReLU\n",
    "        dZ1 = np.dot(self.parameters['W2'].T, dZ2) * drelu_dz1\n",
    "        grads['dW1'] = (1/m) * np.dot(dZ1, A0.T)\n",
    "        grads['db1'] = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    def update_parameters(self, grads, learning_rate):\n",
    "        \"\"\"Atualiza os pesos e vieses usando o gradiente descendente.\"\"\"\n",
    "        for l in range(1, len(self.layer_dims)):\n",
    "            self.parameters['W' + str(l)] -= learning_rate * grads['dW' + str(l)]\n",
    "            self.parameters['b' + str(l)] -= learning_rate * grads['db' + str(l)]\n",
    "\n",
    "    def train(self, X, Y, num_iterations=2000, learning_rate=0.01, print_cost=False):\n",
    "        \"\"\"O loop de treinamento principal que une todos os passos.\"\"\"\n",
    "        costs = []\n",
    "        for i in range(num_iterations):\n",
    "            A2, cache = self.forward_propagation(X)\n",
    "            cost = self.compute_cost(A2, Y)\n",
    "            grads = self.backward_propagation(Y, cache)\n",
    "            self.update_parameters(grads, learning_rate)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                costs.append(cost)\n",
    "                if print_cost:\n",
    "                    print(f\"Custo após iteração {i}: {cost:.6f}\")\n",
    "        return costs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Faz previsões em novos dados (0 ou 1).\"\"\"\n",
    "        A2, _ = self.forward_propagation(X)\n",
    "        predictions = (A2 > 0.5).astype(int)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 3: Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a arquitetura da rede\n",
    "# Camada de entrada: 7 features (n_x)\n",
    "# Camada oculta: 5 neurônios (n_h)\n",
    "# Camada de saída: 1 neurônio (n_y)\n",
    "n_x = X_train_T.shape[0]\n",
    "n_h = 5\n",
    "n_y = 1\n",
    "layer_dimensions = [n_x, n_h, n_y]\n",
    "\n",
    "# Definir hiperparâmetros\n",
    "iterations = 2500\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Criar e treinar o modelo\n",
    "nn = NeuralNetwork(layer_dims=layer_dimensions)\n",
    "costs = nn.train(X_train_T, y_train_T, num_iterations=iterations, learning_rate=learning_rate, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(np.arange(0, iterations, 100), costs)\n",
    "plt.title(\"Curva de Custo Durante o Treinamento\", fontsize=18)\n",
    "plt.xlabel(\"Iterações\", fontsize=14)\n",
    "plt.ylabel(\"Custo (Entropia Cruzada)\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 4: Avaliação de Desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_T = nn.predict(X_test_T)\n",
    "y_pred = y_pred_T.flatten() # Aplainar de (1, 180) para (180,)\n",
    "\n",
    "# Calcular métricas de avaliação\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Métricas de Avaliação do Modelo\")\n",
    "print(f\"Acurácia: {accuracy:.4f}\")\n",
    "print(f\"Precisão: {precision:.4f}\")\n",
    "print(f\"Recall (Sensibilidade): {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de Confusão\n",
    "\n",
    "A matriz de confusão ajuda a visualizar o desempenho do classificador, mostrando os acertos e erros para cada classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, annot_kws={\"size\": 16})\n",
    "plt.title('Matriz de Confusão', fontsize=18)\n",
    "plt.ylabel('Classe Verdadeira', fontsize=14)\n",
    "plt.xlabel('Classe Prevista', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva ROC e AUC\n",
    "\n",
    "A curva ROC (Receiver Operating Characteristic) mede a capacidade do modelo de distinguir entre as classes. A AUC (Area Under the Curve) quantifica essa capacidade: quanto mais perto de 1, melhor o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para a curva ROC, precisamos das probabilidades, não das classes 0/1\n",
    "y_pred_proba_T, _ = nn.forward_propagation(X_test_T)\n",
    "y_pred_proba = y_pred_proba_T.flatten()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(fpr, tpr, color='green', lw=2.5, label=f'Curva ROC (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de Falsos Positivos (FPR)', fontsize=14)\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos (TPR)', fontsize=14)\n",
    "plt.title('Curva ROC (Modelo Feito à Mão)', fontsize=18)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "A rede neural implementada manualmente demonstrou um desempenho excelente, com métricas de avaliação fortes e uma alta pontuação de AUC. Isso valida que a nossa implementação dos componentes fundamentais (forward/backward propagation, gradiente descendente) foi bem-sucedida e é capaz de resolver eficazmente o problema de classificação proposto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
